#!/usr/bin/env python3
"""
Enhanced CVE Exploit Finder with Predictive Scoring
Casts wider net and provides exploit probability scores for manual verification
"""

import requests
import time
import re
import os
import sys
import json
import openpyxl
from urllib.parse import quote, unquote
from bs4 import BeautifulSoup
from datetime import datetime
import urllib3
import colorama
from colorama import Fore, Style, Back
from openpyxl.styles import Font, PatternFill, Border, Side, Alignment
from openpyxl.utils import get_column_letter

# Initialize colorama
colorama.init(autoreset=True)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EnhancedCVEExploitFinder:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.results = []
        self.delay = 0.5
        
        # High-confidence exploit keywords
        self.exploit_keywords = [
            'exploit', 'poc', 'proof-of-concept', 'vulnerability', 'payload',
            'metasploit', 'shell', 'remote code execution', 'rce', 'buffer overflow',
            'sql injection', 'xss', 'csrf', 'lfi', 'rfi', 'privilege escalation',
            'backdoor', 'trojan', 'malware', 'zero-day', '0day', 'security advisory'
        ]
        
        # File extensions that might contain exploits
        self.exploit_extensions = [
            '.py', '.rb', '.pl', '.sh', '.ps1', '.c', '.cpp', '.java', '.php',
            '.asp', '.aspx', '.jsp', '.html', '.js', '.vbs', '.bat', '.cmd'
        ]
        
        # Security-focused domains (expanded list)
        self.security_domains = [
            'github.com', 'exploit-db.com', 'packetstormsecurity.com',
            'infosecinstitute.com', 'medium.com', 'rhinosecuritylabs.com',
            'rapid7.com', 'tenable.com', 'nmap.org', 'seclists.org',
            'cxsecurity.com', 'vulners.com', 'securityfocus.com',
            'hackerone.com', 'bugcrowd.com', '0day.today', 'seebug.org',
            'vuldb.com', 'cvedetails.com', 'nvd.nist.gov', 'mitre.org',
            'exploit.linuxsec.org', 'exploitalert.com', 'exploits.shodan.io',
            'security.snyk.io', 'vulndb.cyberriskanalytics.com', 'zerodayinitiative.com',
            'blog.rapid7.com', 'www.secureworks.com', 'research.checkpoint.com',
            'labs.f-secure.com', 'blog.trendmicro.com', 'unit42.paloaltonetworks.com',
            'threatpost.com', 'krebsonsecurity.com', 'darkreading.com',
            'securityweek.com', 'bleepingcomputer.com', 'thehackernews.com'
        ]
    
    def print_header(self, text):
        print(Fore.CYAN + Style.BRIGHT + f"\n{text}")
        print(Fore.CYAN + "-" * len(text))
    
    def print_success(self, text):
        print(Fore.GREEN + Style.BRIGHT + text)
    
    def print_warning(self, text):
        print(Fore.YELLOW + Style.BRIGHT + text)
    
    def print_error(self, text):
        print(Fore.RED + Style.BRIGHT + text)
    
    def print_info(self, text):
        print(Fore.BLUE + Style.BRIGHT + text)
    
    def calculate_exploit_probability(self, url, title="", description="", cve=""):
        """Calculate probability that a URL contains an exploit (0-100)"""
        score = 0
        url_lower = url.lower()
        title_lower = title.lower()
        desc_lower = description.lower()
        combined_text = f"{url_lower} {title_lower} {desc_lower}"
        
        # Base score for security domains
        for domain in self.security_domains:
            if domain in url_lower:
                if domain in ['exploit-db.com', '0day.today', 'seebug.org']:
                    score += 40  # High confidence domains
                elif domain in ['github.com', 'packetstormsecurity.com']:
                    score += 25  # Medium-high confidence
                else:
                    score += 15  # Medium confidence
                break
        
        # CVE presence in URL
        cve_variants = [
            cve.lower(),
            cve.replace('-', '').lower(),
            cve.replace('CVE-', '').replace('-', '').lower(),
            cve.split('-')[1] + cve.split('-')[2] if len(cve.split('-')) >= 3 else ''
        ]
        
        for variant in cve_variants:
            if variant and variant in url_lower:
                score += 20
                break
        
        # Exploit keywords in URL/title/description
        keyword_matches = 0
        for keyword in self.exploit_keywords:
            if keyword in combined_text:
                keyword_matches += 1
        
        if keyword_matches >= 3:
            score += 25
        elif keyword_matches >= 2:
            score += 15
        elif keyword_matches >= 1:
            score += 10
        
        # File extensions
        for ext in self.exploit_extensions:
            if url_lower.endswith(ext):
                score += 15
                break
        
        # Specific URL patterns
        exploit_patterns = [
            '/exploits/', '/exploit/', '/poc/', '/vulnerability/',
            '/advisory/', '/security/', '/cve-', '/disclosure/',
            '/exploit-', '/proof-of-concept', 'metasploit'
        ]
        
        for pattern in exploit_patterns:
            if pattern in url_lower:
                score += 10
        
        # GitHub specific scoring
        if 'github.com' in url_lower:
            if '/blob/' in url_lower or '/tree/' in url_lower:
                score += 10  # Specific file/directory
            if any(word in url_lower for word in ['exploit', 'poc', 'cve', 'vulnerability']):
                score += 15
        
        # Penalty for generic pages
        penalty_patterns = [
            '/search', '/category/', '/tag/', '/author/', '/archive/',
            '/feed/', '/rss/', '/login', '/signup', '/about', '/contact'
        ]
        
        for pattern in penalty_patterns:
            if pattern in url_lower:
                score -= 20
                break
        
        # Cap at 100
        return min(100, max(0, score))
    
    def search_google_comprehensive(self, cve):
        """Comprehensive Google search with multiple query variations"""
        try:
            all_urls = []
            search_queries = [
                f'"{cve}" exploit',
                f'"{cve}" proof of concept',
                f'"{cve}" poc',
                f'"{cve}" vulnerability exploit',
                f'"{cve}" metasploit',
                f'{cve} site:github.com',
                f'{cve} site:exploit-db.com',
                f'{cve} site:packetstormsecurity.com',
                f'{cve} site:medium.com',
                f'{cve} exploit filetype:py',
                f'{cve} exploit filetype:rb',
                f'{cve} "remote code execution"',
                f'{cve} "buffer overflow"',
                f'{cve} payload',
                f'inurl:"{cve}"'
            ]
            
            for query in search_queries[:8]:  # Limit to prevent rate limiting
                try:
                    search_url = f"https://www.google.com/search?q={quote(query)}&num=20"
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    }
                    
                    response = requests.get(search_url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # Extract search result URLs
                        for result in soup.find_all('div', class_='g'):
                            link_elem = result.find('a')
                            if link_elem and link_elem.get('href'):
                                url = link_elem['href']
                                if url.startswith('http') and 'google.com' not in url:
                                    # Get title and snippet
                                    title_elem = result.find('h3')
                                    title = title_elem.text if title_elem else ""
                                    
                                    snippet_elem = result.find('span', class_='aCOpRe')
                                    if not snippet_elem:
                                        snippet_elem = result.find('div', class_='s')
                                    snippet = snippet_elem.text if snippet_elem else ""
                                    
                                    probability = self.calculate_exploit_probability(url, title, snippet, cve)
                                    
                                    all_urls.append({
                                        'url': url,
                                        'title': title,
                                        'snippet': snippet,
                                        'probability': probability,
                                        'source': 'Google'
                                    })
                    
                    time.sleep(2)  # Respect rate limits
                except Exception as e:
                    self.print_warning(f"    Google query failed: {query[:30]}...")
                    continue
            
            return all_urls
        except Exception as e:
            self.print_error(f"      Google Search Error: {e}")
            return []
    
    def search_exploit_db_comprehensive(self, cve):
        """Enhanced Exploit-DB search"""
        try:
            exploits = []
            
            # Multiple search approaches
            search_urls = [
                f"https://www.exploit-db.com/search?cve={cve}",
                f"https://www.exploit-db.com/search?q={cve}",
                f"https://www.exploit-db.com/search?q={cve.replace('CVE-', '')}"
            ]
            
            for search_url in search_urls:
                try:
                    response = self.session.get(search_url, timeout=15)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # Find exploit links
                        for link in soup.find_all('a', href=True):
                            href = link['href']
                            if '/exploits/' in href:
                                full_url = f"https://www.exploit-db.com{href}" if href.startswith('/') else href
                                title = link.text.strip()
                                
                                probability = self.calculate_exploit_probability(full_url, title, "", cve)
                                
                                exploits.append({
                                    'url': full_url,
                                    'title': title,
                                    'snippet': 'Exploit-DB entry',
                                    'probability': probability,
                                    'source': 'Exploit-DB'
                                })
                    
                    time.sleep(0.5)
                except:
                    continue
            
            return exploits
        except Exception as e:
            self.print_error(f"      Exploit-DB Error: {e}")
            return []
    
    def search_github_comprehensive(self, cve):
        """Comprehensive GitHub search"""
        try:
            github_results = []
            
            # Repository search
            repo_queries = [
                f"{cve} exploit",
                f"{cve} poc",
                f"{cve} vulnerability",
                cve
            ]
            
            for query in repo_queries[:2]:  # Limit API calls
                try:
                    search_url = f"https://api.github.com/search/repositories?q={quote(query)}&sort=updated&per_page=20"
                    response = self.session.get(search_url, timeout=10)
                    
                    if response.status_code == 200:
                        data = response.json()
                        for repo in data.get('items', []):
                            url = repo['html_url']
                            title = repo['name']
                            description = repo.get('description', '')
                            
                            probability = self.calculate_exploit_probability(url, title, description, cve)
                            
                            github_results.append({
                                'url': url,
                                'title': title,
                                'snippet': description,
                                'probability': probability,
                                'source': 'GitHub-Repo'
                            })
                    
                    time.sleep(1)
                except:
                    continue
            
            # Code search
            try:
                code_search_url = f"https://api.github.com/search/code?q={quote(cve)}&per_page=15"
                response = self.session.get(code_search_url, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    for item in data.get('items', []):
                        url = item['html_url']
                        title = item['name']
                        
                        probability = self.calculate_exploit_probability(url, title, "", cve)
                        
                        github_results.append({
                            'url': url,
                            'title': title,
                            'snippet': f"Code file: {item['path']}",
                            'probability': probability,
                            'source': 'GitHub-Code'
                        })
            except:
                pass
            
            return github_results
        except Exception as e:
            self.print_error(f"      GitHub Error: {e}")
            return []
    
    def search_packetstorm_comprehensive(self, cve):
        """Enhanced PacketStorm search"""
        try:
            results = []
            
            search_queries = [
                cve,
                cve.replace('CVE-', ''),
                f"{cve} exploit"
            ]
            
            for query in search_queries[:2]:
                try:
                    search_url = f"https://packetstormsecurity.com/search/?q={quote(query)}"
                    response = self.session.get(search_url, timeout=15)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # Find all relevant links
                        for link in soup.find_all('a', href=True):
                            href = link['href']
                            if any(path in href for path in ['/files/', '/exploits/', '/papers/']):
                                full_url = f"https://packetstormsecurity.com{href}" if href.startswith('/') else href
                                title = link.text.strip()
                                
                                probability = self.calculate_exploit_probability(full_url, title, "", cve)
                                
                                results.append({
                                    'url': full_url,
                                    'title': title,
                                    'snippet': 'PacketStorm Security',
                                    'probability': probability,
                                    'source': 'PacketStorm'
                                })
                    
                    time.sleep(0.5)
                except:
                    continue
            
            return results
        except Exception as e:
            self.print_error(f"      PacketStorm Error: {e}")
            return []
    
    def search_nvd_references(self, cve):
        """Get all references from NVD"""
        try:
            results = []
            
            search_url = f"https://services.nvd.nist.gov/rest/json/cves/2.0?cveId={cve}"
            response = self.session.get(search_url, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                for vuln in data.get('vulnerabilities', []):
                    for ref in vuln.get('cve', {}).get('references', []):
                        url = ref.get('url', '')
                        if url:
                            probability = self.calculate_exploit_probability(url, "", "", cve)
                            
                            results.append({
                                'url': url,
                                'title': 'NVD Reference',
                                'snippet': 'Official CVE reference',
                                'probability': probability,
                                'source': 'NVD'
                            })
            
            return results
        except Exception as e:
            self.print_error(f"      NVD Error: {e}")
            return []
    
    def search_additional_sources(self, cve):
        """Search additional security sources"""
        try:
            results = []
            
            # Additional search engines and sources
            additional_searches = [
                f"https://www.bing.com/search?q={quote(f'{cve} exploit')}&count=20",
                f"https://duckduckgo.com/?q={quote(f'{cve} poc')}&t=h_"
            ]
            
            # You can add more specific site searches here
            site_searches = [
                f'site:vulners.com "{cve}"',
                f'site:cxsecurity.com "{cve}"',
                f'site:0day.today "{cve}"',
                f'site:seebug.org "{cve}"'
            ]
            
            # For now, we'll use a simple approach
            # In a production environment, you might want to implement
            # specific scrapers for each site
            
            return results
        except Exception as e:
            self.print_error(f"      Additional Sources Error: {e}")
            return []
    
    def search_all_sources_comprehensive(self, cve):
        """Comprehensive search across all sources"""
        self.print_header(f"🔍 Comprehensive Search for {cve}")
        
        all_results = []
        
        # Search Google
        self.print_info("  🔍 Searching Google (comprehensive)...")
        google_results = self.search_google_comprehensive(cve)
        self.print_success(f"    ✓ Google: Found {len(google_results)} results")
        all_results.extend(google_results)
        
        # Search Exploit-DB
        self.print_info("  🔍 Searching Exploit-DB (comprehensive)...")
        exploitdb_results = self.search_exploit_db_comprehensive(cve)
        self.print_success(f"    ✓ Exploit-DB: Found {len(exploitdb_results)} results")
        all_results.extend(exploitdb_results)
        
        # Search GitHub
        self.print_info("  🔍 Searching GitHub (comprehensive)...")
        github_results = self.search_github_comprehensive(cve)
        self.print_success(f"    ✓ GitHub: Found {len(github_results)} results")
        all_results.extend(github_results)
        
        # Search PacketStorm
        self.print_info("  🔍 Searching PacketStorm (comprehensive)...")
        packetstorm_results = self.search_packetstorm_comprehensive(cve)
        self.print_success(f"    ✓ PacketStorm: Found {len(packetstorm_results)} results")
        all_results.extend(packetstorm_results)
        
        # Search NVD References
        self.print_info("  🔍 Searching NVD References...")
        nvd_results = self.search_nvd_references(cve)
        self.print_success(f"    ✓ NVD: Found {len(nvd_results)} results")
        all_results.extend(nvd_results)
        
        # Remove duplicates based on URL
        seen_urls = set()
        unique_results = []
        
        for result in all_results:
            url = result['url']
            if url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
        
        # Sort by probability (highest first)
        unique_results.sort(key=lambda x: x['probability'], reverse=True)
        
        # Categorize results by probability
        high_prob = [r for r in unique_results if r['probability'] >= 70]
        medium_prob = [r for r in unique_results if 40 <= r['probability'] < 70]
        low_prob = [r for r in unique_results if r['probability'] < 40]
        
        self.print_success(f"  ✅ Total unique URLs found: {len(unique_results)}")
        self.print_success(f"  🎯 High probability (70%+): {len(high_prob)}")
        self.print_warning(f"  ⚠️  Medium probability (40-69%): {len(medium_prob)}")
        self.print_info(f"  ℹ️  Low probability (<40%): {len(low_prob)}")
        
        # Display top results
        self.print_info(f"\n  🏆 Top 10 Results for {cve}:")
        for i, result in enumerate(unique_results[:10], 1):
            prob_color = Fore.GREEN if result['probability'] >= 70 else Fore.YELLOW if result['probability'] >= 40 else Fore.RED
            self.print_info(f"    {i}. {prob_color}[{result['probability']}%]{Style.RESET_ALL} {result['url']}")
            if result['title']:
                self.print_info(f"       📝 {result['title'][:80]}...")
        
        return {
            'cve': cve,
            'total_urls': len(unique_results),
            'high_probability': len(high_prob),
            'medium_probability': len(medium_prob),
            'low_probability': len(low_prob),
            'results': unique_results,
            'timestamp': datetime.now().isoformat()
        }
    
    def read_scope_file(self, filename='scope.txt'):
        """Read CVEs from scope file"""
        if not os.path.exists(filename):
            self.print_error(f"❌ Error: {filename} not found!")
            return []
        
        cves = []
        try:
            with open(filename, 'r') as f:
                for line in f:
                    line = line.strip()
                    # Find CVE patterns
                    cve_matches = re.findall(r'CVE-\d{4}-\d+', line.upper())
                    cves.extend(cve_matches)
            
            unique_cves = list(dict.fromkeys(cves))
            self.print_success(f"📋 Found {len(unique_cves)} unique CVEs in {filename}")
            return unique_cves
            
        except Exception as e:
            self.print_error(f"❌ Error reading {filename}: {e}")
            return []
    
    def create_comprehensive_excel_report(self, results, output_file='comprehensive_cve_exploit_report.xlsx'):
        """Create comprehensive Excel report with all URLs and probability scores"""
        try:
            wb = openpyxl.Workbook()
            
            # Summary Sheet
            summary_ws = wb.active
            summary_ws.title = "Summary"
            
            # Header styles
            header_font = Font(bold=True, color="FFFFFF")
            header_fill = PatternFill(start_color="1F4E78", end_color="1F4E78", fill_type="solid")
            thin_border = Border(
                left=Side(style='thin'), right=Side(style='thin'), 
                top=Side(style='thin'), bottom=Side(style='thin')
            )
            
            # Summary headers
            summary_headers = ["CVE", "Total URLs", "High Prob (70%+)", "Medium Prob (40-69%)", "Low Prob (<40%)", "Top URL", "Top Score"]
            
            for col_num, header in enumerate(summary_headers, 1):
                cell = summary_ws.cell(row=1, column=col_num, value=header)
                cell.font = header_font
                cell.fill = header_fill
                cell.border = thin_border
            
            # Summary data
            for row_num, result in enumerate(results, 2):
                summary_ws.cell(row=row_num, column=1, value=result['cve'])
                summary_ws.cell(row=row_num, column=2, value=result['total_urls'])
                summary_ws.cell(row=row_num, column=3, value=result['high_probability'])
                summary_ws.cell(row=row_num, column=4, value=result['medium_probability'])
                summary_ws.cell(row=row_num, column=5, value=result['low_probability'])
                
                if result['results']:
                    top_result = result['results'][0]
                    summary_ws.cell(row=row_num, column=6, value=top_result['url'])
                    summary_ws.cell(row=row_num, column=7, value=f"{top_result['probability']}%")
            
            # Auto-adjust column widths
            for col in range(1, 8):
                summary_ws.column_dimensions[get_column_letter(col)].width = 20
            
            # Detailed Results Sheet
            detail_ws = wb.create_sheet("Detailed Results")
            detail_headers = ["CVE", "URL", "Title", "Source", "Probability %", "Category", "Snippet"]
            
            for col_num, header in enumerate(detail_headers, 1):
                cell = detail_ws.cell(row=1, column=col_num, value=header)
                cell.font = header_font
                cell.fill = header_fill
                cell.border = thin_border
            
            # Detailed data
            detail_row = 2
            for result in results:
                cve = result['cve']
                for url_result in result['results']:
                    detail_ws.cell(row=detail_row, column=1, value=cve)
                    detail_ws.cell(row=detail_row, column=2, value=url_result['url'])
                    detail_ws.cell(row=detail_row, column=3, value=url_result['title'])
                    detail_ws.cell(row=detail_row, column=4, value=url_result['source'])
                    detail_ws.cell(row=detail_row, column=5, value=url_result['probability'])
                    
                    # Category based on probability
                    if url_result['probability'] >= 70:
                        category = "HIGH"
                        fill_color = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
                    elif url_result['probability'] >= 40:
                        category = "MEDIUM"
                        fill_color = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")
                    else:
                        category = "LOW"
                        fill_color = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
                    
                    category_cell = detail_ws.cell(row=detail_row, column=6, value=category)
                    category_cell.fill = fill_color
                    
                    detail_ws.cell(row=detail_row, column=7, value=url_result['snippet'][:100])
                    
                    detail_row += 1
            
            # Auto-adjust column widths
            detail_ws.column_dimensions['A'].width = 15  # CVE
            detail_ws.column_dimensions['B'].width = 60  # URL
            detail_ws.column_dimensions['C'].width = 30  # Title
            detail_ws.column_dimensions['D'].width = 15  # Source
            detail_ws.column_dimensions['E'].width = 12  # Probability
            detail_ws.column_dimensions['F'].width = 10  # Category
            detail_ws.column_dimensions['G'].width = 40  # Snippet
            
            # Statistics Sheet
            stats_ws = wb.create_sheet("Statistics")
            
            # Calculate overall statistics
            total_cves = len(results)
            total_urls = sum(r['total_urls'] for r in results)
            total_high = sum(r['high_probability'] for r in results)
            total_medium = sum(r['medium_probability'] for r in results)
            total_low = sum(r['low_probability'] for r in results)
            
            stats_data = [
                ("Metric", "Value"),
                ("Total CVEs Processed", total_cves),
                ("Total URLs Found", total_urls),
                ("High Probability URLs (70%+)", total_high),
                ("Medium Probability URLs (40-69%)", total_medium),
                ("Low Probability URLs (<40%)", total_low),
                ("Average URLs per CVE", f"{total_urls/total_cves:.1f}" if total_cves > 0 else "0"),
                ("High Probability Rate", f"{(total_high/total_urls)*100:.1f}%" if total_urls > 0 else "0%"),
                ("Report Generated", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            ]
            
            for row_num, (metric, value) in enumerate(stats_data, 1):
                if row_num == 1:  # Header
                    cell1 = stats_ws.cell(row=row_num, column=1, value=metric)
                    cell2 = stats_ws.cell(row=row_num, column=2, value=value)
                    cell1.font = header_font
                    cell2.font = header_font
                    cell1.fill = header_fill
                    cell2.fill = header_fill
                else:
                    stats_ws.cell(row=row_num, column=1, value=metric)
                    stats_ws.cell(row=row_num, column=2, value=value)
            
            stats_ws.column_dimensions['A'].width = 25
            stats_ws.column_dimensions['B'].width = 20
            
            # Save workbook
            wb.save(output_file)
            self.print_success(f"💾 Comprehensive Excel report saved to {output_file}")
            
        except Exception as e:
            self.print_error(f"❌ Error creating Excel report: {e}")
    
    def run(self):
        """Main execution"""
        self.print_header("🚀 Enhanced CVE Exploit Finder with Predictive Scoring")
        
        print(Fore.CYAN + Style.BRIGHT + """
╔══════════════════════════════════════════════════════════════╗
║            Enhanced CVE Exploit Finder v2.0                 ║
║         Comprehensive Search with Probability Scoring       ║
╚══════════════════════════════════════════════════════════════╝
        """)
        
        # Read CVEs from scope file
        cves = self.read_scope_file()
        if not cves:
            self.print_error("❌ No CVEs found. Please create a scope.txt file with CVE IDs.")
            return
        
        self.print_success(f"🎯 Processing {len(cves)} CVEs...")
        
        all_results = []
        
        for i, cve in enumerate(cves, 1):
            try:
                self.print_info(f"\n📍 Processing CVE {i}/{len(cves)}: {cve}")
                
                # Comprehensive search
                cve_results = self.search_all_sources_comprehensive(cve)
                all_results.append(cve_results)
                
                # Progress update
                if len(cve_results['results']) > 0:
                    self.print_success(f"  ✅ {cve}: Found {len(cve_results['results'])} total URLs")
                else:
                    self.print_warning(f"  ⚠️  {cve}: No URLs found")
                
                # Rate limiting between CVEs
                if i < len(cves):
                    time.sleep(1)
                
            except KeyboardInterrupt:
                self.print_warning("\n⚠️  Interrupted by user. Saving partial results...")
                break
            except Exception as e:
                self.print_error(f"❌ Error processing {cve}: {e}")
                continue
        
        # Generate reports
        self.print_header("📊 Generating Reports")
        
        # Console summary
        self.print_console_summary(all_results)
        
        # Save JSON report
        self.save_json_report(all_results)
        
        # Create Excel report
        self.create_comprehensive_excel_report(all_results)
        
        # Save high-confidence URLs to text file
        self.save_high_confidence_urls(all_results)
        
        self.print_success("\n🎉 All reports generated successfully!")
        self.print_info("📋 Check the following files:")
        self.print_info("  • comprehensive_cve_exploit_report.xlsx (Full Excel report)")
        self.print_info("  • cve_results.json (JSON data)")
        self.print_info("  • high_confidence_exploits.txt (High probability URLs)")
    
    def print_console_summary(self, results):
        """Print detailed console summary"""
        self.print_header("📈 Final Summary")
        
        total_cves = len(results)
        total_urls = sum(r['total_urls'] for r in results)
        total_high = sum(r['high_probability'] for r in results)
        total_medium = sum(r['medium_probability'] for r in results)
        total_low = sum(r['low_probability'] for r in results)
        
        # Overall statistics
        print(f"{Fore.CYAN}{'='*60}")
        print(f"{Fore.GREEN}🎯 OVERALL STATISTICS:")
        print(f"{Fore.WHITE}   Total CVEs Processed: {Fore.YELLOW}{total_cves}")
        print(f"{Fore.WHITE}   Total URLs Found: {Fore.YELLOW}{total_urls}")
        print(f"{Fore.GREEN}   High Probability (70%+): {Fore.YELLOW}{total_high}")
        print(f"{Fore.YELLOW}   Medium Probability (40-69%): {Fore.YELLOW}{total_medium}")
        print(f"{Fore.RED}   Low Probability (<40%): {Fore.YELLOW}{total_low}")
        
        if total_cves > 0:
            print(f"{Fore.WHITE}   Average URLs per CVE: {Fore.YELLOW}{total_urls/total_cves:.1f}")
        if total_urls > 0:
            print(f"{Fore.WHITE}   High Confidence Rate: {Fore.YELLOW}{(total_high/total_urls)*100:.1f}%")
        
        print(f"{Fore.CYAN}{'='*60}")
        
        # Top performers
        self.print_info("\n🏆 TOP CVEs BY EXPLOIT PROBABILITY:")
        sorted_results = sorted(results, key=lambda x: x['high_probability'], reverse=True)
        
        for i, result in enumerate(sorted_results[:10], 1):
            if result['high_probability'] > 0:
                print(f"  {i:2d}. {Fore.GREEN}{result['cve']:<15}{Style.RESET_ALL} "
                      f"High: {Fore.GREEN}{result['high_probability']:2d}{Style.RESET_ALL} "
                      f"Medium: {Fore.YELLOW}{result['medium_probability']:2d}{Style.RESET_ALL} "
                      f"Total: {result['total_urls']:3d}")
        
        # CVEs with no results
        no_results = [r for r in results if r['total_urls'] == 0]
        if no_results:
            self.print_warning(f"\n⚠️  CVEs with no results found ({len(no_results)}):")
            for result in no_results:
                print(f"     • {result['cve']}")
    
    def save_json_report(self, results, filename='cve_results.json'):
        """Save detailed JSON report"""
        try:
            report_data = {
                'metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'total_cves': len(results),
                    'total_urls': sum(r['total_urls'] for r in results),
                    'version': '2.0'
                },
                'results': results
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            self.print_success(f"💾 JSON report saved to {filename}")
            
        except Exception as e:
            self.print_error(f"❌ Error saving JSON report: {e}")
    
    def save_high_confidence_urls(self, results, filename='high_confidence_exploits.txt'):
        """Save high-confidence URLs to text file for manual verification"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("HIGH CONFIDENCE EXPLOIT URLs (70%+ Probability)\n")
                f.write("="*60 + "\n")
                f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                for result in results:
                    cve = result['cve']
                    high_confidence_urls = [r for r in result['results'] if r['probability'] >= 70]
                    
                    if high_confidence_urls:
                        f.write(f"\n{cve} ({len(high_confidence_urls)} high-confidence URLs):\n")
                        f.write("-" * 50 + "\n")
                        
                        for i, url_result in enumerate(high_confidence_urls, 1):
                            f.write(f"{i:2d}. [{url_result['probability']:3d}%] {url_result['url']}\n")
                            if url_result['title']:
                                f.write(f"    Title: {url_result['title']}\n")
                            f.write(f"    Source: {url_result['source']}\n")
                            if url_result['snippet']:
                                f.write(f"    Snippet: {url_result['snippet'][:100]}...\n")
                            f.write("\n")
                
                # Summary section
                f.write("\n" + "="*60 + "\n")
                f.write("SUMMARY FOR MANUAL VERIFICATION:\n")
                f.write("="*60 + "\n")
                
                total_high = sum(r['high_probability'] for r in results)
                f.write(f"Total high-confidence URLs to verify: {total_high}\n")
                
                # Group by source
                source_counts = {}
                for result in results:
                    for url_result in result['results']:
                        if url_result['probability'] >= 70:
                            source = url_result['source']
                            source_counts[source] = source_counts.get(source, 0) + 1
                
                f.write("\nBy Source:\n")
                for source, count in sorted(source_counts.items(), key=lambda x: x[1], reverse=True):
                    f.write(f"  {source}: {count} URLs\n")
            
            self.print_success(f"📝 High-confidence URLs saved to {filename}")
            
        except Exception as e:
            self.print_error(f"❌ Error saving high-confidence URLs: {e}")

def main():
    """Main function"""
    try:
        finder = EnhancedCVEExploitFinder()
        finder.run()
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}⚠️  Script interrupted by user")
    except Exception as e:
        print(f"\n{Fore.RED}❌ Fatal error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()